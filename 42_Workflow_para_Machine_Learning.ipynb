{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPBBV61NYcPmIXSiDvPNliP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JhonnyLimachi/Sigmoidal/blob/main/42_Workflow_para_Machine_Learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img alt=\"Colaboratory logo\" width=\"15%\" src=\"https://raw.githubusercontent.com/carlosfab/escola-data-science/master/img/novo_logo_bg_claro.png\">\n",
        "\n",
        "#### **Data Science na Prática 3.0**\n",
        "*by [sigmoidal.ai](https://sigmoidal.ai)*\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "6kvkv-ST2C5M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Workflow para Machine Learning\n",
        "\n",
        "<center><img src=\"https://images.unsplash.com/photo-1533749871411-5e21e14bcc7d?ixid=MnwxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8&ixlib=rb-1.2.1&auto=format&fit=crop&w=1471&q=80\" width=\"60%\"></center>\n",
        "\n",
        "O *workflow* para problemas de Machine Learning apresentado pelo Google, é composto de 6 etapas principais:\n",
        "\n",
        "1. Adquirir os Dados\n",
        "2. Explorar os Dados\n",
        "3. Preparar os Dados\n",
        "4. Construir, Treinar e Avaliar seu Modelo\n",
        "5. Otimizar os Hiperparâmetros\n",
        "6. Deploy do Modelo\n",
        "\n",
        "Há ainda uma etapa `2.5 Escolher o Modelo`, que não faz parte dos *frameworks* tradicionais, mas que merece destaque devido a sua importância crítica.\n",
        "\n",
        "Para conhecer mais sobre esse workflow, visite o [guideline da Google](https://developers.google.com/machine-learning/guides/text-classification).\n",
        "\n",
        "<center><img src=\"https://raw.githubusercontent.com/carlosfab/dsnp2/master/img/Workflow-2.png\" width=\"700\"></center>\n"
      ],
      "metadata": {
        "id": "GLYe8GsA2hnz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Aquisição dos dados\n",
        "\n",
        "<center><img src=\"https://images.unsplash.com/photo-1528582654826-585a71fcf7f4?ixid=MnwxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8&ixlib=rb-1.2.1&auto=format&fit=crop&w=1470&q=80\" width=\"50%\"></center>\n",
        "\n",
        "De onde virão os seus dados? De um `csv`, `xls`, de um banco de dados `mysql`, consumindo microserviços, fazendo web scraping ou usando APIs disponibilizadas por empresas como Twitter, RD Station e Google?\n",
        "\n",
        "Se for por meio de APIs, quais são as regras para não ser bloqueado? O consumo de dados é gratuito? Precisa criar uma chave?\n",
        "\n",
        "Se for por meio das planilhas da sua empresa, você sabe se elas têm dependências, se alguém pode alterar a lógica ou corromper o arquivo? Os lançamentos são manuais, por meio de campos abertos, ou fechados?\n",
        "\n",
        "Os dados estão minimamente balanceados?\n",
        "\n",
        "As informações representam estatisticamente todas as situações possíveis?\n",
        "\n",
        "Depois de importar os dados, você realizou cheques para ver se eles estão consistentes?"
      ],
      "metadata": {
        "id": "w10nMn3W2luS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exemplo 1**"
      ],
      "metadata": {
        "id": "Dkw6SQ6n28ea"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "5by1tTfB1kP_"
      },
      "outputs": [],
      "source": [
        "def load_imdb_sentiment_analysis_dataset(data_path, seed=123):\n",
        "    \"\"\"Loads the IMDb movie reviews sentiment analysis dataset.\n",
        "\n",
        "    # Arguments\n",
        "        data_path: string, path to the data directory.\n",
        "        seed: int, seed for randomizer.\n",
        "\n",
        "    # Returns\n",
        "        A tuple of training and validation data.\n",
        "        Number of training samples: 25000\n",
        "        Number of test samples: 25000\n",
        "        Number of categories: 2 (0 - negative, 1 - positive)\n",
        "\n",
        "    # References\n",
        "        Mass et al., http://www.aclweb.org/anthology/P11-1015\n",
        "\n",
        "        Download and uncompress archive from:\n",
        "        http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
        "    \"\"\"\n",
        "    imdb_data_path = os.path.join(data_path, 'aclImdb')\n",
        "\n",
        "    # Load the training data\n",
        "    train_texts = []\n",
        "    train_labels = []\n",
        "    for category in ['pos', 'neg']:\n",
        "        train_path = os.path.join(imdb_data_path, 'train', category)\n",
        "        for fname in sorted(os.listdir(train_path)):\n",
        "            if fname.endswith('.txt'):\n",
        "                with open(os.path.join(train_path, fname)) as f:\n",
        "                    train_texts.append(f.read())\n",
        "                train_labels.append(0 if category == 'neg' else 1)\n",
        "\n",
        "    # Load the validation data.\n",
        "    test_texts = []\n",
        "    test_labels = []\n",
        "    for category in ['pos', 'neg']:\n",
        "        test_path = os.path.join(imdb_data_path, 'test', category)\n",
        "        for fname in sorted(os.listdir(test_path)):\n",
        "            if fname.endswith('.txt'):\n",
        "                with open(os.path.join(test_path, fname)) as f:\n",
        "                    test_texts.append(f.read())\n",
        "                test_labels.append(0 if category == 'neg' else 1)\n",
        "\n",
        "    # Shuffle the training data and labels.\n",
        "    random.seed(seed)\n",
        "    random.shuffle(train_texts)\n",
        "    random.seed(seed)\n",
        "    random.shuffle(train_labels)\n",
        "\n",
        "    return ((train_texts, np.array(train_labels)),\n",
        "            (test_texts, np.array(test_labels)))"
      ]
    }
  ]
}